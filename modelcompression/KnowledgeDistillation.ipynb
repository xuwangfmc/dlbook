{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.13"
    },
    "colab": {
      "name": "KnowledgeDistillation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xuwangfmc/dlbook/blob/main/modelcompression/KnowledgeDistillation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a13ef812"
      },
      "source": [
        "# 知识蒸馏\n",
        "\n",
        "知识蒸馏（Knowledge Distillation）的思想是用原始高精度的大模型指导小模型进行训练，最后只采用小模型，从而使得保持一定精度的同时减少了参数。知识蒸馏主要分为logits学习和features学习两种。Logits方法的上限较低，而feature的方法与模型结构和任务本身强相关。\n",
        "\n",
        "![KnowledgeDistillation.png](https://s2.loli.net/2022/01/22/LoZBtdq1AnzbhwX.png)\n",
        "\n",
        "该教程主要介绍知识蒸馏有效的原因，以及在实际运用中如何用大模型指导小模型进行训练。\n"
      ],
      "id": "a13ef812"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 知识蒸馏有效的原因  \n",
        "- 例如当data不是很干净的时候，对一般的model来说就是noise,只会干扰学习。透过去学习其它大model预测的logits会比较好。  \n",
        "-label和label之间可能有关联，这可以引导小model去学习。例如数字5可能与4，6有关系。  \n",
        "-弱化已经学习不错的target，避免让其gradient干扰其它还没学好的task。  "
      ],
      "metadata": {
        "id": "5Np6Wn2TvrL-"
      },
      "id": "5Np6Wn2TvrL-"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 知识蒸馏后的模型有时会比原来的模型效果还要好的原因\n",
        "因为常见的标签是one-hot的，而TeacherNet给到的是含有其它可能性的标签。即如果mnist中’1’的图片，我们人为打标签，会标记为{‘1’:1.0}, 而TeacherNet的输出会是{‘1’:0.7,’7’:0.2,’9’:0.1}等形式，从而让StudentNet学到一些暗知识。\n",
        "在实际操作中，对于TeacherNet最后的softmax，通常会进行Temperature操作，原来的softmax通过自然指数后输出接近one-hot的向量，而Teaperature则会软化输出接近one-hot的特性，从而让StudentNet学习到暗知识，如下图所示，Temperature通常取大于1的值，数值T越接近0，概率最大类别输出越接近1，其它越接近零。\n",
        "知识蒸馏中“蒸馏”的意思就是在训练时用较大Temperature，让StudentNet学习到TeacherNet的分布。训练结束后，StudentNet又用T=1，即经典的softmax输出。Temperature的由高变低就是抽象的“蒸馏”过程。"
      ],
      "metadata": {
        "id": "uMrlr6uvvujl"
      },
      "id": "uMrlr6uvvujl"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 实战实例\n",
        "Loss构建为Teacher与Student之间的KL散度，加上Student与Ground Truth的损失，用$\\alpha$调整权重。\n",
        "$Loss = \\alpha T^2 \\times KL(\\frac{\\text{Teacher's Logits}}{T} || \\frac{\\text{Student's Logits}}{T}) + (1-\\alpha)(\\text{Original Loss})$ "
      ],
      "metadata": {
        "id": "Ulls3XeMvylK"
      },
      "id": "Ulls3XeMvylK"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4xglr30u724"
      },
      "source": [
        "步骤1：加载数据集"
      ],
      "id": "Y4xglr30u724"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4G1oFxLPu4cq",
        "outputId": "c14b5089-4ee3-4ba5-bea1-7990cada5be6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Download dataset\n",
        "!gdown --id '1O6pFYd9aw1cZbry-NXk3k3tTXLVgssIg' --output food-11.zip\n",
        "# Unzip the files\n",
        "!unzip -q food-11.zip"
      ],
      "id": "4G1oFxLPu4cq",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1O6pFYd9aw1cZbry-NXk3k3tTXLVgssIg\n",
            "To: /content/food-11.zip\n",
            "100% 277M/277M [00:01<00:00, 148MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qG8ZKUU_u_Of"
      },
      "source": [
        "步骤2：加载StudentNet"
      ],
      "id": "qG8ZKUU_u_Of"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XxkHsJLkvPol",
        "outputId": "90670f21-81f5-4650-e0e2-42761ac8b622",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import torch\n",
        "import os\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models as models\n",
        "# 运行之前的Architecture_Design文件\n",
        "!gdown --id '1-sSaAOk3vnmfZv8F4Vo_YhdrHkTNn7bh' --output \"Architecture_Design.ipynb\"\n",
        "%run \"Architecture_Design.ipynb\""
      ],
      "id": "XxkHsJLkvPol",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-sSaAOk3vnmfZv8F4Vo_YhdrHkTNn7bh\n",
            "To: /content/Architecture_Design.ipynb\n",
            "\r  0% 0.00/7.62k [00:00<?, ?B/s]\r100% 7.62k/7.62k [00:00<00:00, 12.4MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkBADPHR1DYP"
      },
      "source": [
        "步骤3：执行代码"
      ],
      "id": "zkBADPHR1DYP"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "050c9718",
        "outputId": "6f64388d-6dc3-45b8-f3e7-0f98546bdd7c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import torchvision.models as models\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "# 下载TeacherNet的参数\n",
        "!gdown --id '1B8ljdrxYXJsZv2vmTequdPOofp3VF3NN' --output teacher_resnet18.bin\n",
        "class FoodDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, dir_path, transform, cuda=False):\n",
        "        self.cuda = cuda\n",
        "        self.transform = transform\n",
        "        self.x = []\n",
        "        self.y = []\n",
        "        img_names = sorted(os.listdir(dir_path))\n",
        "        for img_name in img_names:  # glob返回匹配到的所有文件的路径\n",
        "            img_path = os.path.join(dir_path, img_name)\n",
        "            label = int(img_name.split(\"_\")[0])\n",
        "\n",
        "            image = Image.open(img_path)\n",
        "            # Get File Descriptor\n",
        "            image_fp = image.fp\n",
        "            image.load()\n",
        "            # Close File Descriptor (or it'll reach OPEN_MAX)\n",
        "            image_fp.close()\n",
        "\n",
        "            self.x.append(image)\n",
        "            self.y.append(label)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.transform(self.x[idx])\n",
        "        label = torch.torch.tensor(self.y[idx], dtype=torch.int64)\n",
        "        if self.cuda:\n",
        "            image = image.cuda()\n",
        "            label = label.cuda()\n",
        "        return image, label\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.x)\n",
        "\n",
        "\n",
        "trainTransform = transforms.Compose([\n",
        "    transforms.RandomCrop(256, pad_if_needed=True, padding_mode='symmetric'),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "testTransform = transforms.Compose([\n",
        "    transforms.CenterCrop(256),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "\n",
        "def get_dataloader(dir_path='/data',mode='training', batch_size=32, cuda=False):\n",
        "\n",
        "    assert mode in ['training', 'testing', 'validation']\n",
        "\n",
        "    dataset = FoodDataset(\n",
        "        f'{dir_path}',\n",
        "        transform=trainTransform if mode == 'training' else testTransform, cuda=cuda)\n",
        "\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=(mode == 'training'))\n",
        "\n",
        "    return dataloader\n",
        "\n",
        "# config\n",
        "batch_size = 4\n",
        "cuda = True\n",
        "epochs = 15\n",
        "\n",
        "\n",
        "\n",
        "# 定义Knowledge Distillation中的损失函数\n",
        "def loss_fn_kd(outputs, labels, teacher_outputs, T=20, alpha=0.5):\n",
        "    # 一般的Cross Entropy\n",
        "    hard_loss = F.cross_entropy(outputs, labels) * (1. - alpha)\n",
        "    # 让student的logits做log_softmax后对目标概率(teacher的logits/T后softmax)做KL Divergence。\n",
        "    soft_loss = nn.KLDivLoss(reduction='batchmean')(F.log_softmax(outputs/T, dim=1), F.softmax(teacher_outputs/T, dim=1)) * (alpha * T * T)\n",
        "    return hard_loss + soft_loss\n",
        "\n",
        "# 运行一个epoch\n",
        "def run_epoch(dataloader, update=True, alpha=0.5):\n",
        "    total_num, total_hit, total_loss = 0, 0, 0\n",
        "    for now_step, batch_data in enumerate(dataloader):\n",
        "        # 清空梯度\n",
        "        optimizer.zero_grad()\n",
        "        # 获取数据\n",
        "        inputs, hard_labels = batch_data\n",
        "        # Teacher不用反向传播，所以使用torch.no_grad()\n",
        "        with torch.no_grad():\n",
        "            soft_labels = teacher_net(inputs)\n",
        "\n",
        "        if update:\n",
        "            logits = student_net(inputs)\n",
        "            # 使用前面定义的融合soft label&hard label的损失函数：loss_fn_kd，T=20是原论文设定的参数值\n",
        "            loss = loss_fn_kd(logits, hard_labels, soft_labels, 20, alpha)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        else:\n",
        "            # 只是做validation的话，就不用计算梯度\n",
        "            with torch.no_grad():\n",
        "                logits = student_net(inputs)\n",
        "                loss = loss_fn_kd(logits, hard_labels, soft_labels, 20, alpha)\n",
        "\n",
        "        total_hit += torch.sum(torch.argmax(logits, dim=1) == hard_labels).item()\n",
        "        total_num += len(inputs)\n",
        "\n",
        "        total_loss += loss.item() * len(inputs)\n",
        "    return total_loss / total_num, total_hit / total_num\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # 加载数据\n",
        "    train_dataloader = get_dataloader('training','training', batch_size, cuda)\n",
        "    valid_dataloader = get_dataloader('validation','validation', batch_size, cuda)\n",
        "    print('Data Loaded')\n",
        "\n",
        "    # 加载网络\n",
        "    teacher_net = models.resnet18(pretrained=False, num_classes=11)\n",
        "    teacher_net.load_state_dict(torch.load(f'./teacher_resnet18.bin'))\n",
        "    student_net = StudentNet(base=16)\n",
        "    if cuda:\n",
        "        teacher_net = teacher_net.cuda()\n",
        "        student_net = student_net.cuda()\n",
        "    print('Model Loaded')\n",
        "\n",
        "    # 开始训练(Knowledge Distillation)\n",
        "    print('Training Started')\n",
        "    optimizer = optim.AdamW(student_net.parameters(), lr=1e-3)\n",
        "    teacher_net.eval()\n",
        "    now_best_acc = 0\n",
        "    for epoch in range(epochs):\n",
        "        student_net.train()\n",
        "        train_loss, train_acc = run_epoch(train_dataloader, update=True)\n",
        "        student_net.eval()\n",
        "        valid_loss, valid_acc = run_epoch(valid_dataloader, update=False)\n",
        "\n",
        "        # 存下最好的model\n",
        "        if valid_acc > now_best_acc:\n",
        "            now_best_acc = valid_acc\n",
        "            torch.save(student_net.state_dict(), './student_model.bin')\n",
        "        print('epoch {:>3d}: train loss: {:6.4f}, acc {:6.4f} valid loss: {:6.4f}, acc {:6.4f}'.format(\n",
        "            epoch, train_loss, train_acc, valid_loss, valid_acc))\n"
      ],
      "id": "050c9718",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1B8ljdrxYXJsZv2vmTequdPOofp3VF3NN\n",
            "To: /content/teacher_resnet18.bin\n",
            "100% 44.8M/44.8M [00:00<00:00, 123MB/s] \n",
            "Data Loaded\n",
            "Model Loaded\n",
            "Training Started\n",
            "epoch   0: train loss: 20.8721, acc 0.2838 valid loss: 20.6007, acc 0.2108\n",
            "epoch   1: train loss: 20.2215, acc 0.3128 valid loss: 19.5978, acc 0.2719\n",
            "epoch   2: train loss: 19.7559, acc 0.3347 valid loss: 20.3388, acc 0.2911\n",
            "epoch   3: train loss: 18.8262, acc 0.3616 valid loss: 17.9361, acc 0.3276\n",
            "epoch   4: train loss: 18.4149, acc 0.3745 valid loss: 19.0117, acc 0.3303\n",
            "epoch   5: train loss: 18.0903, acc 0.3931 valid loss: 19.5524, acc 0.3312\n",
            "epoch   6: train loss: 18.1428, acc 0.3969 valid loss: 18.5035, acc 0.3239\n",
            "epoch   7: train loss: 17.6772, acc 0.4155 valid loss: 17.8237, acc 0.3485\n",
            "epoch   8: train loss: 17.0190, acc 0.4275 valid loss: 16.4812, acc 0.3932\n",
            "epoch   9: train loss: 16.0832, acc 0.4449 valid loss: 18.4974, acc 0.3859\n",
            "epoch  10: train loss: 15.9349, acc 0.4602 valid loss: 18.2331, acc 0.3887\n",
            "epoch  11: train loss: 15.0920, acc 0.4843 valid loss: 17.5011, acc 0.4069\n",
            "epoch  12: train loss: 14.9271, acc 0.4805 valid loss: 18.6357, acc 0.3859\n",
            "epoch  13: train loss: 14.6179, acc 0.4921 valid loss: 15.6431, acc 0.4398\n",
            "epoch  14: train loss: 14.2779, acc 0.5079 valid loss: 19.3728, acc 0.4206\n"
          ]
        }
      ]
    }
  ]
}